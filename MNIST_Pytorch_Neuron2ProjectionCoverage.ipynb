{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron 2-projection on-off activation coverage (1) - MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by fixing the random seed to ensure reproducability. In all our examples, we use 42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "# Fix the number for repeatability (we have also stored the trained model)\n",
    "numpy.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required library, and set the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.misc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd  import Variable\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.misc import imsave\n",
    "import os\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper-parameters. The parameter \"sizeOfNeuronsToMonitor\" is the number of neurons we will monitor, which is the 2nd to last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "sizeOfNeuronsToMonitor = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then , prepare the training and test set. MNIST can be directly downloaded. Notice that we do not normalize the input data, but just rely on small learning rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data/mnist', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/mnist', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to display MNIST images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "util.displayMNIST(images[0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the network \n",
    "\n",
    "To use run-time monitoring, apart from standard \"forward()\" function, we additionally define another function to return values of intermediate layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    " \n",
    "        self.conv1 = nn.Conv2d(1, 40, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(40, 20, 5)\n",
    "        self.fc1 = nn.Linear(20 * 4 * 4, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80, sizeOfNeuronsToMonitor)\n",
    "        self.fc4 = nn.Linear(sizeOfNeuronsToMonitor, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original 28x28x1 -(conv)-> 24x24x40 -(pool)-> 12x12x40\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Original 12x12x40 -(conv)-> 8x8x20 -(pool)-> 4x4x20\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten it to an array of inputs\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out \n",
    "  \n",
    "    # Here we add another function, which does the same forward computation but also extracts intermediate layer results\n",
    "    def forwardWithIntermediate(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        intermediateValues = x\n",
    "        x = F.relu(x)\n",
    "        out = self.fc4(x)\n",
    "        return out, intermediateValues    \n",
    "    \n",
    "net = NeuralNet()\n",
    "net.eval()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model (if you have a pretrained one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model to be attacked\n",
    "# net = NeuralNet()\n",
    "net.load_state_dict(torch.load('models/1_model_MNIST_CNN.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "furtherTrain = False\n",
    "\n",
    "if furtherTrain: \n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.81 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    outofActivationPattern = 0\n",
    "    outofActivationPatternAndResultWrong = 0\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger 2-projection neuron on-off activation coverage computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.metrics import KProjection\n",
    "\n",
    "k_Value = 1\n",
    "\n",
    "metric = KProjection.Neuron_OnOff_KProjection_Metric(k_Value, sizeOfNeuronsToMonitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the function addInputs() to update the k-projection table based on all visited patterns for each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current input size fed into the metric: 64\n",
      "1-projection neuron on-off activation coverage:77/80=0.9625\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 3264\n",
      "1-projection neuron on-off activation coverage:77/80=0.9625\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 6464\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 9664\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 12864\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 16064\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 19264\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 22464\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 25664\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 28864\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 32064\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 35264\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 38464\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 41664\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 44864\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 48064\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 51264\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 54464\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 57664\n",
      "1-projection neuron on-off activation coverage:78/80=0.975\n",
      "\n",
      "\n",
      "Accuracy of the network on the all train images: 16.468333333333334 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        total = total + (len(labels))\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        \n",
    "        # Add the batch of neuron activation patterns to the k-projection table\n",
    "        metric.addInputs(intermediateValues)\n",
    "                \n",
    "        if(i % 50) == 0:\n",
    "            print('Current input size fed into the metric: '+str(total))\n",
    "            metric.printMetricQuantity()\n",
    "            print(\"\\n\")\n",
    "        i = i+1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.printMetricQuantity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
