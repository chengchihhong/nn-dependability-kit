{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Monitoring Neuron Activation Patterns (1) - MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by fixing the random seed to ensure reproducability. In all our examples, we use 42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "# Fix the number for repeatability (we have also stored the trained model)\n",
    "numpy.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required library, and set the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.misc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd  import Variable\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.misc import imsave\n",
    "import os\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper-parameters. The parameter \"sizeOfNeuronsToMonitor\" is the number of neurons we will monitor, which is the 2nd to last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "sizeOfNeuronsToMonitor = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then , prepare the training and test set. MNIST can be directly downloaded. Notice that we do not normalize the input data, but just rely on small learning rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data/mnist', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/mnist', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to display MNIST images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "util.displayMNIST(images[0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the network \n",
    "\n",
    "To use run-time monitoring, apart from standard \"forward()\" function, we additionally define another function to return values of intermediate layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    " \n",
    "        self.conv1 = nn.Conv2d(1, 40, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(40, 20, 5)\n",
    "        self.fc1 = nn.Linear(20 * 4 * 4, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80, sizeOfNeuronsToMonitor)\n",
    "        self.fc4 = nn.Linear(sizeOfNeuronsToMonitor, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original 28x28x1 -(conv)-> 24x24x40 -(pool)-> 12x12x40\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Original 12x12x40 -(conv)-> 8x8x20 -(pool)-> 4x4x20\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten it to an array of inputs\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out \n",
    "  \n",
    "    # Here we add another function, which does the same forward computation but also extracts intermediate layer results\n",
    "    def forwardWithIntermediate(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        intermediateValues = x\n",
    "        x = F.relu(x)\n",
    "        out = self.fc4(x)\n",
    "        return out, intermediateValues    \n",
    "    \n",
    "net = NeuralNet()\n",
    "net.eval()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model (if you have a pretrained one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model to be attacked\n",
    "# net = NeuralNet()\n",
    "net.load_state_dict(torch.load('models/1_model_MNIST_CNN.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "furtherTrain = False\n",
    "\n",
    "if furtherTrain: \n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.81 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    outofActivationPattern = 0\n",
    "    outofActivationPatternAndResultWrong = 0\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Trigger neuron on-off activation pattern monitoring\n",
    "\n",
    "Here we pretend that test set is the \"real data\" after deployment, so we only record neuron activation patterns inside the training set. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.rv import napmonitor\n",
    "monitor = napmonitor.NAP_Monitor(num_classes, sizeOfNeuronsToMonitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the function addAllNeuronPatternsToClass() to record all visited patterns for each batch (internally the monitor will filter incorrect results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the all train images: 99.34166666666667 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Add the batch of neuron activation patterns to the monitor\n",
    "        monitor.addAllNeuronPatternsToClass(intermediateValues.numpy(), predicted.numpy(), labels.numpy(), -1)\n",
    "\n",
    "                \n",
    "    print('Accuracy of the network on the all train images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform run-time monitoring and examine if an activation pattern has been visited before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.81 %\n",
      "Out-of-activation pattern on the 10000 test images: 7.66 %\n",
      "Out-of-activation pattern & misclassified / out-of-activation pattern : 10.704960835509139 %\n"
     ]
    }
   ],
   "source": [
    "# We don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    outofActivationPattern = 0\n",
    "    outofActivationPatternAndResultWrong = 0\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        # Additional processing for runtime monitoring\n",
    "        \n",
    "        predictedNp = predicted.numpy()\n",
    "        \n",
    "        result = (predicted == labels)\n",
    "        res = result.numpy()\n",
    "               \n",
    "        # Iterate over each image in the batch\n",
    "        for exampleIndex in range(intermediateValues.shape[0]):   \n",
    "            if not monitor.isPatternContained(intermediateValues.numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                outofActivationPattern = outofActivationPattern +1\n",
    "                if res[exampleIndex] == False :\n",
    "                    outofActivationPatternAndResultWrong = outofActivationPatternAndResultWrong + 1\n",
    "          \n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "    print('Out-of-activation pattern on the 10000 test images: {} %'.format(100 * outofActivationPattern / total))\n",
    "    print('Out-of-activation pattern & misclassified / out-of-activation pattern : {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-activation pattern & misclassified / mis-classified: 68.90756302521008 %\n"
     ]
    }
   ],
   "source": [
    "print('Out-of-activation pattern & misclassified / mis-classified: {} %'.format(100 * outofActivationPatternAndResultWrong / (total - correct)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B. Produce adversarial training example using iterative FGSM and check if the activation pattern is within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "\n",
    "util.displayMNIST(images[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative FGSM attack is within util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[0].unsqueeze(0)\n",
    "label = labels[0].unsqueeze(0)\n",
    "\n",
    "adv_img, noise, attackSuccessful = util.iterative_FGSM_attack(img, label, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if attackSuccessful :\n",
    "    util.displayMNIST(adv_img[0].numpy())\n",
    "else:\n",
    "    print(\"iterative FGSM is not effective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the noise value is not within [0, 1]. Therefore, we add each pixel  with 0.5 to display the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if attackSuccessful :\n",
    "    util.displayMNIST(noise[0].numpy()+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the adversarial image produces any similar activation pattern. After flowing the pattern into the monitor, the monitor diagnoses that it has not seen anything similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.6698, -2.0072, -2.4491,  2.9892, -1.5599,  0.6358, -7.5163,\n",
      "          2.5283, -0.9109,  4.8922]])\n"
     ]
    }
   ],
   "source": [
    "out, intermediateValues = net.forwardWithIntermediate(adv_img)        \n",
    "print(out.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adv example is not within activation pattern\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(out.data, 1)\n",
    "#iv = intermediateValues.detach().numpy()\n",
    "#mat = np.zeros(intermediateValues.shape)\n",
    "#ivabs = np.greater(iv, mat)\n",
    "\n",
    "# Only perform analysis when the monitor takes all classes\n",
    "for exampleIndex in range(intermediateValues.detach().numpy().shape[0]):      \n",
    "    if not monitor.isPatternContained(intermediateValues.detach().numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "        print(\"Adv example is not within activation pattern\")\n",
    "    else:\n",
    "        print(\"Adv example is within activation pattern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, try to generate adversarial images for each image of the test set, and check the percentage of being out of patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performAdversarialExamination(loader, monitor):\n",
    "    notInNeuronActivationPattern = 0\n",
    "    totalPerturbed = 0\n",
    "    for images, labels in loader:\n",
    "        # print(list(images.size())[0])\n",
    "        for i in range(list(images.size())[0]):\n",
    "\n",
    "            adv_img, noise, attackSuccessful = util.iterative_FGSM_attack(images[i].unsqueeze(0), labels[i].unsqueeze(0), net)\n",
    "\n",
    "            if (attackSuccessful) :\n",
    "                totalPerturbed = totalPerturbed + 1\n",
    "\n",
    "                out, intermediateValues = net.forwardWithIntermediate(adv_img)  \n",
    "                _, predicted = torch.max(out.data, 1)\n",
    "                # iv = intermediateValues.detach().numpy()\n",
    "                # mat = np.zeros(intermediateValues.shape)\n",
    "                # ivabs = np.greater(iv, mat)\n",
    "\n",
    "                for exampleIndex in range(intermediateValues.shape[0]): \n",
    "                    # if not monitor.isOnOffPatternContained(ivabs[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                    if not monitor.isPatternContained(intermediateValues.detach().numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                        notInNeuronActivationPattern = notInNeuronActivationPattern +1\n",
    "\n",
    "\n",
    "    print('not in neuron activation pattern / all total perturbed: {} %'.format(100 * notInNeuronActivationPattern / totalPerturbed))\n",
    "    print('all total perturbed = '+str(totalPerturbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in neuron activation pattern / all total perturbed: 67.5509337860781 %\n",
      "all total perturbed = 9424\n"
     ]
    }
   ],
   "source": [
    "performAdversarialExamination(test_loader, monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Enlarge the region by allowing perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may now enlarge the memorized pattern by allowing 1-bit flip (thus out-of-activation will drop), and run it again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.enlargeSetByOneBitFluctuation(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.81 %\n",
      "Out-of-activation pattern on the 10000 test images: 2.01 %\n",
      "Out-of-activation pattern & misclassified / out-of-activation pattern : 21.890547263681594 %\n"
     ]
    }
   ],
   "source": [
    "# We don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    outofActivationPattern = 0\n",
    "    outofActivationPatternAndResultWrong = 0\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        # Additional processing for runtime monitoring\n",
    "        \n",
    "        predictedNp = predicted.numpy()\n",
    "        \n",
    "        result = (predicted == labels)\n",
    "        res = result.numpy()\n",
    "               \n",
    "        # Iterate over each image in the batch\n",
    "        for exampleIndex in range(intermediateValues.shape[0]):   \n",
    "            if not monitor.isPatternContained(intermediateValues.numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                outofActivationPattern = outofActivationPattern +1\n",
    "                if res[exampleIndex] == False :\n",
    "                    outofActivationPatternAndResultWrong = outofActivationPatternAndResultWrong + 1\n",
    "          \n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "    print('Out-of-activation pattern on the 10000 test images: {} %'.format(100 * outofActivationPattern / total))\n",
    "    print('Out-of-activation pattern & misclassified / out-of-activation pattern : {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-activation pattern & misclassified / mis-classified: 36.97478991596638 %\n"
     ]
    }
   ],
   "source": [
    "print('Out-of-activation pattern & misclassified / mis-classified: {} %'.format(100 * outofActivationPatternAndResultWrong / (total - correct)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in neuron activation pattern / all total perturbed: 35.643039049235995 %\n",
      "all total perturbed = 9424\n"
     ]
    }
   ],
   "source": [
    "performAdversarialExamination(test_loader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.enlargeSetByOneBitFluctuation(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.81 %\n",
      "Out-of-activation pattern on the 10000 test images: 0.6 %\n",
      "Out-of-activation pattern & misclassified / out-of-activation pattern : 31.666666666666668 %\n"
     ]
    }
   ],
   "source": [
    "# We don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    outofActivationPattern = 0\n",
    "    outofActivationPatternAndResultWrong = 0\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        # Additional processing for runtime monitoring\n",
    "        \n",
    "        predictedNp = predicted.numpy()\n",
    "        \n",
    "        result = (predicted == labels)\n",
    "        res = result.numpy()\n",
    "               \n",
    "        # Iterate over each image in the batch\n",
    "        for exampleIndex in range(intermediateValues.shape[0]):   \n",
    "            if not monitor.isPatternContained(intermediateValues.numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                outofActivationPattern = outofActivationPattern +1\n",
    "                if res[exampleIndex] == False :\n",
    "                    outofActivationPatternAndResultWrong = outofActivationPatternAndResultWrong + 1\n",
    "          \n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "    print('Out-of-activation pattern on the 10000 test images: {} %'.format(100 * outofActivationPattern / total))\n",
    "    print('Out-of-activation pattern & misclassified / out-of-activation pattern : {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-activation pattern & misclassified / mis-classified: 15.966386554621849 %\n"
     ]
    }
   ],
   "source": [
    "print('Out-of-activation pattern & misclassified / mis-classified: {} %'.format(100 * outofActivationPatternAndResultWrong / (total - correct)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in neuron activation pattern / all total perturbed: 16.30942275042445 %\n",
      "all total perturbed = 9424\n"
     ]
    }
   ],
   "source": [
    "performAdversarialExamination(test_loader, monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Trigger runtime neuron interval pattern monitoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the max value for output of each neuron in the layer under analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.97608852 12.64085484 14.45319462 20.14254951 12.10131168  7.20467997\n",
      "  14.01826    15.72563648  7.57931471 12.96555805 13.73955441  3.02725577\n",
      "  12.84442329 12.07688141  0.          8.21488285 19.87311172  0.\n",
      "  10.62583828  8.91005993 10.26775932 10.06238365 11.35547543 15.94989872\n",
      "  11.28468704 12.3040657  14.48820591 14.17330456 11.95500565  4.97704792\n",
      "  17.16381836  0.         13.73954773 12.10352325  3.9682436  11.53234291\n",
      "  25.40776634 14.69727898  9.12612629  5.32893801]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "maximumValues = np.zeros([1, sizeOfNeuronsToMonitor])\n",
    "\n",
    "# We don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "    \n",
    "    for exampleIndex in range(intermediateValues.shape[0]):   \n",
    "        maximumValues = np.maximum(intermediateValues.numpy()[exampleIndex,:], maximumValues)\n",
    "    \n",
    "    print(maximumValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare two threshold for fine-grained monitor, which is based on further partitioning the >0 domain to 0-33% MaxInTraining, 33%-66% MaxInTraining, and >66% MaxInTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = (maximumValues*1)/3\n",
    "threshold2 = (maximumValues*2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "runIntervalMonitor = True\n",
    "\n",
    "from nndependability.rv import nipmonitor\n",
    "intervalmonitor = nipmonitor.NIP_Monitor(num_classes, sizeOfNeuronsToMonitor, threshold1, threshold2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only do it on class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classToAnalyze = 2\n",
    "# Use below if we want to analyze the complete class\n",
    "# classToAnalyze = -1\n",
    "\n",
    "if runIntervalMonitor: \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Add the batch of neuron activation patterns to the monitor, with only the 0-th class\n",
    "            intervalmonitor.addAllNeuronPatternsToClass(intermediateValues.numpy(), predicted.numpy(), labels.numpy(), classToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on interval-based monitor ([0,1,2,3]:= [<=0, 0-33%, 33%-66%, >66%] in max training value: \n",
      "Accuracy of the network on the 10000 test images: 98.81 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if runIntervalMonitor: \n",
    "    \n",
    "    # We don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        outofActivationPattern = 0\n",
    "        outofActivationPatternAndResultWrong = 0\n",
    "\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            labels = labels.to(device)\n",
    "            outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Additional processing for runtime monitoring\n",
    "\n",
    "            predictedNp = predicted.numpy()\n",
    "\n",
    "            result = (predicted == labels)\n",
    "            res = result.numpy()\n",
    "\n",
    "            # Iterate over each image in the batch\n",
    "            for exampleIndex in range(intermediateValues.shape[0]): \n",
    "                if (classToAnalyze == -1) or (predicted.numpy()[exampleIndex] == classToAnalyze): \n",
    "                    if not intervalmonitor.isPatternContained(intermediateValues.numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                        outofActivationPattern = outofActivationPattern +1\n",
    "                        if res[exampleIndex] == False :\n",
    "                            outofActivationPatternAndResultWrong = outofActivationPatternAndResultWrong + 1\n",
    "\n",
    "        print('Result on interval-based monitor ([0,1,2,3]:= [<=0, 0-33%, 33%-66%, >66%] in max training value: ')      \n",
    "        print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-activation pattern & misclassified / out-of-activation pattern for class 2: 1.5337423312883436 %\n"
     ]
    }
   ],
   "source": [
    "if classToAnalyze == -1:\n",
    "        print('Out-of-activation pattern on the 10000 test images: {} %'.format(100 * outofActivationPattern / total))\n",
    "        print('Out-of-activation pattern & misclassified / out-of-activation pattern : {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n",
    "else:\n",
    "        print('Out-of-activation pattern & misclassified / out-of-activation pattern for class '+str(classToAnalyze)+': {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), 'local/model_MNIST_CNN.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements \n",
    "\n",
    "During the development of this jupyter notebook, I got some code from existing github repositories:\n",
    "\n",
    "- PyTorch tutorials as in https://pytorch.org/\n",
    "- https://github.com/Lextal/adv-attacks-pytorch-101/ for implementing iterative FGSM on PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
