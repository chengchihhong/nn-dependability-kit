{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks for front-car selection\n",
    "\n",
    "This jupyter notebook demonstrates how to train a neural network to enable front-car selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/frontcar.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed by the above figure, the input for front-car selection contains two parts:\n",
    "\n",
    "* Bounding boxes of vehicles obtained from object recognition (red rectangle in the figure)\n",
    "* Lane information characterized by points or parameters (e.g., green triangle in the figure)\n",
    "\n",
    "The output of the network shall indicate the vehicle in the front, e.g., the red vehicle being sourounded by a blue rectangle.\n",
    "\n",
    "One should observe that designing such a front-car selection can be a nontrivial task, as things such as area covering is not working - the box of the truck actually intersects a larger area with the triangle. Therefore, in this project, we train a neural network to perform required tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "# For repeatability\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration: Go for GPU if exsists\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image being processed has size 1800x450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE_X = 1800\n",
    "IMAGE_SIZE_Y = 450\n",
    "TARGET_LABEL_QUANTITY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare a data processor for preparing data used in training. The input will be organized as a 2D numpy array with size being \"(numberOfVehiclesBeingTracked + 1) x 4\".\n",
    "\n",
    "* For the first numberOfVehiclesBeingTracked elements, they are used to store bounding boxes. THerefore, this neural network only takes finitely many bounding boxes. \n",
    "* For the last element, it is used to store the 4 inputs of the lane detection algorithm.\n",
    "\n",
    "\n",
    "![title](img/inputformat.jpg)\n",
    "\n",
    "\n",
    "To have better performance, we want all data to be normalized to value between 0 and 1. Apart from dividing centerX with IMAGE_SIZE_X and dividing centerY with IMAGE_SIZE_Y, we have one additional processing step for normalizing the data, where we derive the largest bounding box size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class TargetVehicleSelectionDataProcessor():\n",
    "    \n",
    "    numberOfVehiclesBeingTracked = 20\n",
    "    numberOfOutputClasses = numberOfVehiclesBeingTracked + 1\n",
    "\n",
    "    \n",
    "    def __init__(self, vehicleFile, roadFile, targetVehicleFile):\n",
    "        \n",
    "        \n",
    "        object_labels_file = open(vehicleFile, \"r\")\n",
    "\n",
    "        maxQuantityBoxSize = 0\n",
    "        filename = object_labels_file.readline()\n",
    "    \n",
    "        inputFeaturesList = []\n",
    "        print(\"----- Read bounding box information to get max box (for normalization) -----\")\n",
    "        count = 0\n",
    "        while filename:\n",
    "\n",
    "            labels = ast.literal_eval(object_labels_file.readline())        \n",
    "\n",
    "            # For the 1st axis, the items are object-exist(0,1), centerX, centerY, widthX, widthY\n",
    "            for label in labels:\n",
    "                if label[0] == \"car\" or label[0] == \"truck\":\n",
    "                    widthX = label[2][2]\n",
    "                    widthY = label[2][3]\n",
    "                    if(maxQuantityBoxSize < widthX):\n",
    "                        maxQuantityBoxSize = widthX\n",
    "                    if(maxQuantityBoxSize < widthY):\n",
    "                        maxQuantityBoxSize = widthY\n",
    "            filename = object_labels_file.readline()\n",
    " \n",
    "\n",
    "        print(maxQuantityBoxSize)\n",
    "        print(\"----- Read road information to normalize coordinate (for normalization) -----\")\n",
    "        road_file = open(roadFile, \"r\")\n",
    "       \n",
    "        maxValue = [0,0,0,0,0,0,0]\n",
    "        minValue = [IMAGE_SIZE_X, IMAGE_SIZE_X, IMAGE_SIZE_X, IMAGE_SIZE_X, IMAGE_SIZE_X, IMAGE_SIZE_X, IMAGE_SIZE_X]\n",
    "\n",
    "        while True:\n",
    "            road = road_file.readline()\n",
    "            if road == '':\n",
    "                break\n",
    "            laneMarkings = road.replace(\",\", \" \").split()\n",
    "            for i in [1,3,4,5]:\n",
    "                # print(laneMarkings)\n",
    "                if(float(laneMarkings[i]) < minValue[i]):\n",
    "                    minValue[i] = float(laneMarkings[i])\n",
    "                if(float(laneMarkings[i]) > maxValue[i]):\n",
    "                    maxValue[i] = float(laneMarkings[i])\n",
    "        \n",
    "        \n",
    "        minValueLane = [minValue[1], minValue[3], minValue[4], minValue[5]]\n",
    "        maxValueLane = [maxValue[1], maxValue[3], maxValue[4], maxValue[5]]\n",
    "        intervalLane = [0,0,0,0]\n",
    "        for i in range(4):\n",
    "            intervalLane[i] = maxValueLane[i] - minValueLane[i]\n",
    "        print(minValueLane)\n",
    "        print(maxValueLane)\n",
    "        print(intervalLane)\n",
    "        \n",
    "        road_file.close()\n",
    "        \n",
    "\n",
    "        print(\"----- Read bounding box information & road segmation & target vehicle info to create input data -----\")       \n",
    "        object_labels_file = open(vehicleFile, \"r\")\n",
    "        target_file = open(targetVehicleFile, \"r\")                \n",
    "        road_file = open(roadFile, \"r\")\n",
    "\n",
    "        \n",
    "        # First read the image name\n",
    "        object_labels = object_labels_file.readline()\n",
    "        target = target_file.readline()\n",
    "        \n",
    "        # To store numpy input features\n",
    "        inputFeaturesList = []\n",
    "        # To store target vehicle class\n",
    "        FinalSelectedVehicleList = []\n",
    "        \n",
    "        count = 0\n",
    "        while object_labels:\n",
    "\n",
    "            labels = ast.literal_eval(object_labels_file.readline())\n",
    "            laneMarkings = road_file.readline().replace(\",\", \" \").split()\n",
    "            if(count % 10000 == 0):\n",
    "                print(count)\n",
    "            count = count + 1\n",
    "            \n",
    "            # For the 1st axis, the items are object-exist(0,1), centerX, centerY, widthX, widthY\n",
    "            inputLabel = np.zeros([self.numberOfVehiclesBeingTracked + 1, 4])\n",
    "            centerXList = []\n",
    "            centerYList = [] \n",
    "            \n",
    "            vehicleCount = 0\n",
    "            for label in labels:\n",
    "                if label[0] == \"car\" or label[0] == \"truck\":\n",
    "                    \n",
    "                    centerX = label[2][0] \n",
    "                    centerY = label[2][1]\n",
    "                    # Normalize the value to be the maximum quantity of the box size\n",
    "                    widthX = label[2][2] / maxQuantityBoxSize\n",
    "                    widthY = label[2][3] / maxQuantityBoxSize                 \n",
    "                    # Normalize the value by dividing it with the pixel\n",
    "                    centerX = centerX / IMAGE_SIZE_X\n",
    "                    centerY = centerY / IMAGE_SIZE_Y\n",
    "                    centerXList.append(centerX)\n",
    "                    centerYList.append(centerY)\n",
    "                    \n",
    "                    inputLabel[vehicleCount][0] = centerX\n",
    "                    inputLabel[vehicleCount][1] = centerY\n",
    "                    inputLabel[vehicleCount][2] = widthX\n",
    "                    inputLabel[vehicleCount][3] = widthY\n",
    "                    \n",
    "                    vehicleCount = vehicleCount +1\n",
    "                    \n",
    "            inputLabel[self.numberOfVehiclesBeingTracked][0] = (float(laneMarkings[1]) - minValueLane[0])/ intervalLane[0]\n",
    "            inputLabel[self.numberOfVehiclesBeingTracked][1] = (float(laneMarkings[3]) - minValueLane[1])/ intervalLane[1]\n",
    "            inputLabel[self.numberOfVehiclesBeingTracked][2] = (float(laneMarkings[4]) - minValueLane[2])/ intervalLane[2]\n",
    "            inputLabel[self.numberOfVehiclesBeingTracked][3] = (float(laneMarkings[5]) - minValueLane[3])/ intervalLane[3]\n",
    "\n",
    "                    \n",
    "            inputFeaturesList.append(inputLabel)\n",
    "            # Read again the image name, in order to move on to the next one. \n",
    "            object_labels = object_labels_file.readline()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Read the target file\n",
    "            labels = ast.literal_eval(target_file.readline())                                 \n",
    "            targetLabel = np.zeros([self.numberOfOutputClasses]) # for L1 loss\n",
    "\n",
    "            \n",
    "            hasTarget = False\n",
    "            if not (len(labels) == 0):\n",
    "                if labels[0] == \"car\" or labels[0] == \"truck\":\n",
    "                    centerX = labels[2][0] / IMAGE_SIZE_X\n",
    "                    centerY = labels[2][1] / IMAGE_SIZE_Y\n",
    "\n",
    "                    hasFound = False\n",
    "                    for i in range(len(centerXList)):\n",
    "                        if(centerXList[i] == centerX and centerYList[i] == centerY):\n",
    "                            targetLabel[i] = TARGET_LABEL_QUANTITY                    \n",
    "                            hasFound = True\n",
    "                            break\n",
    "                            \n",
    "                    hasTarget = True\n",
    "           \n",
    "            if hasTarget == False:\n",
    "                targetLabel[self.numberOfOutputClasses - 1] = TARGET_LABEL_QUANTITY\n",
    "                \n",
    "            FinalSelectedVehicleList.append(targetLabel)\n",
    "            \n",
    "            # Read again the target file, in order to move on to the next one. \n",
    "            filename = target_file.readline()\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.inputFeatures = np.dstack(inputFeaturesList)\n",
    "        self.inputFeatures = np.rollaxis(self.inputFeatures,-1)\n",
    "        self.inputFeatures = torch.from_numpy(np.float32(self.inputFeatures))\n",
    "        \n",
    "        \n",
    "        self.labels = np.dstack(FinalSelectedVehicleList)\n",
    "        self.labels = np.rollaxis(self.labels,-1)\n",
    "        self.labels = torch.from_numpy(np.float32(np.squeeze(self.labels, axis=1)))\n",
    "  \n",
    "        object_labels_file.close()\n",
    "        road_file.close()        \n",
    "        target_file.close()           \n",
    "\n",
    "\n",
    "        if not(self.inputFeatures.shape[0] == self.labels.shape[0]):\n",
    "            raise Error(\"Size of target vehicles not the same as size of inputs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputFeatures.shape[0] # of how many examples(images?) you have            \n",
    "    \n",
    "    def dumpDataToNpy(self, fileName):\n",
    "        np.save(fileName, self.inputFeatures.numpy())\n",
    "        \n",
    "    def dumpLabelToNpy(self, fileName):\n",
    "        np.save(fileName, self.labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have the .npy file available, please set the variable processRawData to True. Also, prepare a \"local\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processRawData = False\n",
    "\n",
    "if processRawData:\n",
    "    dataset = TargetVehicleSelectionDataProcessor(\"data/batch_results_sorted.txt\", \"data/triangles_opencv_v8.txt\", \"data/fw_label_sorted_v8.txt\")\n",
    "    dataset.dumpDataToNpy(\"local/batch_results_processed_nogrid_v8.npy\")\n",
    "    dataset.dumpLabelToNpy(\"local/fw_label_processed_nogrid_v8.npy\")\n",
    "    print(\"----- size of the data -----\")\n",
    "    print(len(dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then one can prepare dataset using the below class. The parameter \"isL2Loss\" is used to create output either as one hot encoding (as used in L1 loss) or as general numeric enocding (as used in L2 loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetVehicleSelectionDataset(Dataset):\n",
    "    \n",
    "    inputFeatures = None\n",
    "    labels = None\n",
    "    \n",
    "    def __init__(self, vehicleFileNpy, targetVehicleFileNpy, isL2Loss):     \n",
    "\n",
    "        self.inputFeatures = torch.from_numpy(np.load(vehicleFileNpy))\n",
    "        \n",
    "        if isL2Loss:\n",
    "            # Rewrite one-hot encoding back\n",
    "            self.labels = torch.from_numpy(np.argmax(np.int64(np.load(targetVehicleFileNpy)), axis=1))\n",
    "        else:\n",
    "            self.labels = torch.from_numpy(np.load(targetVehicleFileNpy))\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # stuff\n",
    "        return (self.inputFeatures[index], self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputFeatures.shape[0] # of how many examples(images?) you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123996\n"
     ]
    }
   ],
   "source": [
    "dataset = TargetVehicleSelectionDataset(\"local/batch_results_processed_nogrid_v8.npy\", \"local/fw_label_processed_nogrid_v8.npy\", True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare a simple ReLU network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (fc1): Linear(in_features=84, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "numberOfVehiclesBeingTracked = 20\n",
    "numberOfOutputClasses = numberOfVehiclesBeingTracked + 1\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear((numberOfVehiclesBeingTracked+1)*4, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, numberOfOutputClasses)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, (numberOfVehiclesBeingTracked+1)*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out \n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "net = NeuralNet()\n",
    "net.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "batch_size = 2048\n",
    "num_epochs = 40\n",
    "\n",
    "# weight_decay as parameter to be tuned 0, 0.001, 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.0001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the parameter from a pre-trained network (the network is not trained with very high correctness rate; you may further train it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('models/model_front_car_100x50.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We resere 5% for validation set (for hyper parameter tuning), 5% for test set (never touch it), and 90% for training. The data is shuffled then distributed, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "test_size = 0.05\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(test_size * num_train))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx, test_idx = indices[(2*split):], indices[split:(2*split)], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the training, validation, and test set based on the sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, sampler=train_sampler,\n",
    "                                           batch_size=batch_size)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=dataset, sampler=valid_sampler,\n",
    "                                           batch_size=batch_size)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset, sampler=test_sampler,\n",
    "                                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to train the network, change the variable \"furtherTrain\" to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "furtherTrain = False\n",
    "\n",
    "if furtherTrain: \n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            if batch_size > 5000:\n",
    "                if (i+1) % 4 == 0:\n",
    "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))    \n",
    "            elif batch_size > 1000:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))           \n",
    "            else: \n",
    "                if (i+1) % 1000 == 0:\n",
    "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the training set: 86.50603057402462 %\n",
      "Accuracy of the network on the validation set: 86.33650588804646 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "isL2Loss = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #print(images[0])\n",
    "        outputs = net(images)\n",
    "        _, predictedY = torch.max(outputs.data, 1)\n",
    "        \n",
    "        correctLabelsY = None        \n",
    "        if(isL2Loss):\n",
    "            # L2 (without one-hot encoding)\n",
    "            correctLabelsY = labels.data            \n",
    "        else:\n",
    "            # L1 (with one-hot encoding)\n",
    "            _, correctLabelsY = torch.max(labels.data, 1)\n",
    "\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # print(labels.numpy().shape)\n",
    "        correct += (predictedY == correctLabelsY).sum().item()\n",
    "        \n",
    "\n",
    "    print('Accuracy of the network on the training set: {} %'.format(100 * correct / total))\n",
    "     \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = net(images)\n",
    "        _, predictedY = torch.max(outputs.data, 1)\n",
    "        \n",
    "        correctLabelsY = None        \n",
    "        if(isL2Loss):\n",
    "            # L2 (without one-hot encoding)\n",
    "            correctLabelsY = labels.data            \n",
    "        else:\n",
    "            # L1 (with one-hot encoding)\n",
    "            _, correctLabelsY = torch.max(labels.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # print(labels.numpy().shape)\n",
    "        correct += (predictedY == correctLabelsY).sum().item()\n",
    "        \n",
    "\n",
    "    print('Accuracy of the network on the validation set: {} %'.format(100 * correct / total))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), 'models/model_front_car_100x100x50.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the front-car selection network on images\n",
    "\n",
    "Here we provide two utility functions \n",
    "* derivePredictedFrontCar() for deriving the index of the bounding box to be front car\n",
    "* drawFrontCar() for drawing the front car in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivePredictedFrontCar(boundingBoxList, laneDetectionInfo):\n",
    "    '''\n",
    "    Derive which box is the front vehicle. \n",
    "    \n",
    "    Args:\n",
    "    boundingBoxList -- a list of bounding boxes (represented as an array [centerX, centerY, sizeX, sizeY]) representing vehicles (other objects such as signs shall be removed)\n",
    "    laneDetectionInfo -- a list of 4 inputs [x1, x2, y2, x3]\n",
    "    \n",
    "    Returns:\n",
    "    int: index of boundingBoxList if one of them is a predicted front car, -1 if no car is detected\n",
    "    '''\n",
    "    \n",
    "    # Parameters here are derived by analyzing the data\n",
    "    maxQuantityBoxSize = 1984.615966796875\n",
    "    minValueLane = [268.0, 708.0, 7.0, 954.0]\n",
    "    intervalLane = [569.0, 409.0, 292.0, 602.0]\n",
    "    numberOfVehiclesBeingTracked = 20\n",
    "    numberOfOutputClasses = numberOfVehiclesBeingTracked + 1    \n",
    "    \n",
    "    inputLabel = np.zeros([numberOfVehiclesBeingTracked + 1, 4])\n",
    "    \n",
    "    vehicleCount = 0\n",
    "    for boundingBox in boundingBoxList:\n",
    "        centerX = boundingBox[0] \n",
    "        centerY = boundingBox[1]\n",
    "        # Normalize the value to be the maximum quantity of the box size\n",
    "        widthX = boundingBox[2] / maxQuantityBoxSize\n",
    "        widthY = boundingBox[3] / maxQuantityBoxSize                 \n",
    "\n",
    "        \n",
    "        # Normalize the value by dividing it with the pixel\n",
    "        centerX = centerX / IMAGE_SIZE_X\n",
    "        centerY = centerY / IMAGE_SIZE_Y\n",
    "\n",
    "        inputLabel[vehicleCount][0] = centerX\n",
    "        inputLabel[vehicleCount][1] = centerY\n",
    "        inputLabel[vehicleCount][2] = widthX\n",
    "        inputLabel[vehicleCount][3] = widthY\n",
    "        vehicleCount = vehicleCount + 1\n",
    "        inputLabel[numberOfVehiclesBeingTracked][0] = (float(laneDetectionInfo[0]) - minValueLane[0])/ intervalLane[0]\n",
    "        inputLabel[numberOfVehiclesBeingTracked][1] = (float(laneDetectionInfo[1]) - minValueLane[1])/ intervalLane[1]\n",
    "        inputLabel[numberOfVehiclesBeingTracked][2] = (float(laneDetectionInfo[2]) - minValueLane[2])/ intervalLane[2]\n",
    "        inputLabel[numberOfVehiclesBeingTracked][3] = (float(laneDetectionInfo[3]) - minValueLane[3])/ intervalLane[3]\n",
    "        \n",
    "\n",
    "    inputs = []\n",
    "    inputs.append(inputLabel)         \n",
    "    inputs = np.dstack(inputs)\n",
    "    inputs = np.rollaxis(inputs,-1)\n",
    "    inputs = torch.from_numpy(np.float32(inputs))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # print(inputs[0])\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)    \n",
    "\n",
    "        output = predicted.numpy()[0]\n",
    "        \n",
    "        if(output == numberOfOutputClasses - 1):\n",
    "            print(\"network says that no front car\")\n",
    "            return -1\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawFrontCar() is within util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Image frames0023291.jpg\n",
    "\n",
    "(1) Input from the bounding box\n",
    "\n",
    "[('car', 0.9984562397003174, (576.3399047851562, 316.2917785644531, 286.28106689453125, 186.20248413085938)), \n",
    "('car', 0.9743680953979492, (127.26703643798828, 286.13848876953125, 252.41139221191406, 164.10342407226562)), \n",
    "('car', 0.8326014876365662, (781.6996459960938, 238.186279296875, 42.52123260498047, 26.923107147216797)), \n",
    "('car', 0.8239439129829407, (845.2855834960938, 235.84620666503906, 41.73268127441406, 35.90092468261719)), \n",
    "('car', 0.6865452527999878, (975.0291137695312, 240.38128662109375, 67.97051239013672, 43.96461868286133)), \n",
    "('car', 0.6862249374389648, (699.9054565429688, 240.95892333984375, 52.11764907836914, 34.9674186706543)), \n",
    "('car', 0.5572288036346436, (888.8295288085938, 233.42330932617188, 25.159809112548828, 25.35500144958496))]\n",
    "\n",
    "(2) Input from the lane info: (frames0023291: 297,450 844,189 1203,450)\n",
    "\n",
    "\n",
    "(3) Correct result of frames0023291.jpg (as ground truth):\n",
    "('car', 0.9984562397003174, (576.3399047851562, 316.2917785644531, 286.28106689453125, 186.20248413085938))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- ![title](img/sequence/test/frames0023291.jpg) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBoxList=[[576.3399047851562, 316.2917785644531, 286.28106689453125, 186.20248413085938], \n",
    "                [127.26703643798828, 286.13848876953125, 252.41139221191406, 164.10342407226562],\n",
    "                [781.6996459960938, 238.186279296875, 42.52123260498047, 26.923107147216797],\n",
    "                [845.2855834960938, 235.84620666503906, 41.73268127441406, 35.90092468261719],\n",
    "                [975.0291137695312, 240.38128662109375, 67.97051239013672, 43.96461868286133],\n",
    "                [699.9054565429688, 240.95892333984375, 52.11764907836914, 34.9674186706543],\n",
    "                [888.8295288085938, 233.42330932617188, 25.159809112548828, 25.35500144958496]]\n",
    "\n",
    "laneDetectionInfo=[297,844,189, 1203]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = derivePredictedFrontCar(boundingBoxList, laneDetectionInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index != -1:\n",
    "    util.drawFrontCar('img/sequence/test/frames0023291.jpg', boundingBoxList[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Image frames0060931.jpg\n",
    "\n",
    "(1) Input from the bounding box\n",
    "\n",
    "[('truck', 0.9686129093170166, (664.022216796875, 176.36538696289062, 238.4475555419922, 192.21405029296875)), \n",
    "('truck', 0.8795585036277771, (1045.208251953125, 200.95474243164062, 127.88365173339844, 149.4367218017578)), \n",
    "('car', 0.8282604217529297, (881.839111328125, 175.23509216308594, 29.620193481445312, 24.062349319458008)), \n",
    "('car', 0.7414387464523315, (806.3452758789062, 178.2257080078125, 46.19411849975586, 32.87610626220703)), \n",
    "('car', 0.6790100336074829, (916.25244140625, 170.0439453125, 17.430356979370117, 15.947772979736328))]\n",
    "\n",
    "(2) Input from the lane info: (frames0060931: 469,450 933,171 1195,450)\n",
    "\n",
    "(3) Correct result of frames0060931.jpg:\n",
    "('car', 0.6790100336074829, (916.25244140625, 170.0439453125, 17.430356979370117, 15.947772979736328))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---  ![title](img/sequence/test/frames0060931.jpg) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "boundingBoxList=[[664.022216796875, 176.36538696289062, 238.4475555419922, 192.21405029296875], \n",
    "                [1045.208251953125, 200.95474243164062, 127.88365173339844, 149.4367218017578],\n",
    "                [881.839111328125, 175.23509216308594, 29.620193481445312, 24.062349319458008],\n",
    "                [806.3452758789062, 178.2257080078125, 46.19411849975586, 32.87610626220703],\n",
    "                [916.25244140625, 170.0439453125, 17.430356979370117, 15.947772979736328]]\n",
    "\n",
    "laneDetectionInfo=[469,933,171, 1195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = derivePredictedFrontCar(boundingBoxList, laneDetectionInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index != -1:\n",
    "    util.drawFrontCar('img/sequence/test/frames0060931.jpg', boundingBoxList[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Image frames0061374.jpg\n",
    "\n",
    "(1) Input from the bounding box\n",
    "\n",
    "[('truck', 0.7765164971351624, (912.59716796875, 162.4954071044922, 73.30638885498047, 82.89049530029297)), \n",
    "('car', 0.5375076532363892, (819.02392578125, 178.01235961914062, 35.7996940612793, 28.5452823638916))]\n",
    "\n",
    "\n",
    "(2) Input from the lane info: (frames0061374: 471,916,165, 1216)\n",
    "\n",
    "\n",
    "(3) Correct result of frames0061374.jpg:\n",
    "('truck', 0.7765164971351624, (912.59716796875, 162.4954071044922, 73.30638885498047, 82.89049530029297))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- ![title](img/sequence/test/frames0061374.jpg) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundingBoxList=[[912.59716796875, 162.4954071044922, 73.30638885498047, 82.89049530029297], \n",
    "                [819.02392578125, 178.01235961914062, 35.7996940612793, 28.5452823638916]]\n",
    "\n",
    "laneDetectionInfo=[471,916,165, 1216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = derivePredictedFrontCar(boundingBoxList, laneDetectionInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index != -1:\n",
    "    util.drawFrontCar('img/sequence/test/frames0061374.jpg', boundingBoxList[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run formal verification, it is required to install pulp (https://github.com/coin-or/pulp), which contains CBC MILP solver binding. The installation is simple: \n",
    "\n",
    "```sh\n",
    "$ pip install pulp\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Dataflow analysis (with box domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we trigger dataflow analysis (i.e., abstract interpretation with box abstraction), which computes a sound [min.max] value for each neuron output, based on the assumption that the input is bounded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.formal import staticanalysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of our data, as we have them normalized, the input is within [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputMinBound = np.zeros((numberOfVehiclesBeingTracked+1)*4)\n",
    "inputMaxBound = np.ones((numberOfVehiclesBeingTracked+1)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy constraints: if risk property is not specified, then the verified result is the bound of each output neuron. \n",
    "#inputConstraints = []\n",
    "#riskProperty = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The risk property we want to avoid is the following: \n",
    "\n",
    "* If the  x<sub>i</sub>-th box is empty (placed inside inputConstraints), \n",
    "* Then the output of the network is  x<sub>i</sub> (i.e.,  x<sub>i</sub> >  x<sub>j</sub>, for all j != i) - to be placed in riskProperty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take input i = 5, then the in20, in21, in22, in23 (the associated input related to 5) shall be set to 0 in the input constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputConstraints = [[0, \"==\", 1, \"in20\"], [0, \"==\", 1, \"in21\"], [0, \"==\", 1, \"in22\"], [0, \"==\", 1, \"in23\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riskProperty  = [[0, \"<=\", 1, \"out5\", -1, \"out0\"], [0, \"<=\", 1, \"out5\", -1, \"out1\"], [0, \"<=\", 1, \"out5\", -1, \"out2\"],\n",
    "                 [0, \"<=\", 1, \"out5\", -1, \"out3\"], [0, \"<=\", 1, \"out5\", -1, \"out4\"], [0, \"<=\", 1, \"out5\", -1, \"out6\"],\n",
    "                 [0, \"<=\", 1, \"out5\", -1, \"out7\"], [0, \"<=\", 1, \"out5\", -1, \"out8\"], [0, \"<=\", 1, \"out5\", -1, \"out9\"],\n",
    "                [0, \"<=\", 1, \"out5\", -1, \"out10\"], [0, \"<=\", 1, \"out5\", -1, \"out11\"], [0, \"<=\", 1, \"out5\", -1, \"out12\"],\n",
    "                [0, \"<=\", 1, \"out5\", -1, \"out13\"], [0, \"<=\", 1, \"out5\", -1, \"out14\"], [0, \"<=\", 1, \"out5\", -1, \"out15\"],\n",
    "                 [0, \"<=\", 1, \"out5\", -1, \"out16\"], [0, \"<=\", 1, \"out5\", -1, \"out17\"], [0, \"<=\", 1, \"out5\", -1, \"out18\"],\n",
    "                 [0, \"<=\", 1, \"out5\", -1, \"out19\"], [0, \"<=\", 1, \"out5\", -1, \"out20\"]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform static analysis using boxed domain, and the result indicates that the risk property is never reached, meaning that the network will never output 5 if there is no vehicle in the 5th input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minBound, maxBound = staticanalysis.verify(inputMinBound, inputMaxBound, net, True, inputConstraints, riskProperty)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Dataflow analysis (with octagon domain)\n",
    "\n",
    "After we perform dataflow analysis over the box domain, we can further do analysis on the octagon domain. In the octagon domain, one additionally keeps the min and max value of x<sub>i</sub> - x<sub>j</sub>. Therefore, if there are n neurons in the layer, there will be n(n-1)/2 such entries, meaning that MILP shall be triggered in frequency square to the number of neurons in a layer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.1. Boxed abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Boxed abstraction] Processing layer fc1\n",
      "[Boxed abstraction] Processing layer fc2\n",
      "[Boxed abstraction] Processing layer fc3\n",
      "Completed\n",
      "\n",
      "\n",
      "===== Output bound based on boxed abstraction =====\n",
      "Min:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Max:\n",
      "[315.3517     302.50557    247.08038    312.1524     223.23891\n",
      " 283.24444    268.17951    170.71916    188.12205    160.12929\n",
      " 170.40604    171.89758    187.84121    134.32688    144.17865\n",
      "  93.816854     0.54822098   0.           0.           0.\n",
      " 195.16936   ]\n"
     ]
    }
   ],
   "source": [
    "minBound, maxBound = staticanalysis.verify(inputMinBound, inputMaxBound, net, True, [], [])\n",
    "\n",
    "print(\"===== Output bound based on boxed abstraction =====\")\n",
    "print(\"Min:\")\n",
    "print(minBound)\n",
    "print(\"Max:\")\n",
    "print(maxBound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2: Octagon abstraction \n",
    "We run dataflow analysis with this time adding octagon constraints (do it with caution - it takes longer time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Boxed abstraction] Processing layer fc1\n",
      "[Boxed abstraction] Processing layer fc2\n",
      "[Boxed abstraction] Processing layer fc3\n",
      "[Octagon abstraction] Processing layer fc1\n",
      "\t[0]: 5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[1]: 5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[2]: 5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[3]: 5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[4]: 5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[5]: 10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[6]: 10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[7]: 10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[8]: 10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[9]: 10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[10]: 15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[11]: 15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[12]: 15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[13]: 15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[14]: 15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[15]: 20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[16]: 20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[17]: 20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[18]: 20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[19]: 20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[20]: 25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[21]: 25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[22]: 25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[23]: 25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[24]: 25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[25]: 30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[26]: 30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[27]: 30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[28]: 30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[29]: 30,35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[30]: 35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[31]: 35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[32]: 35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[33]: 35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[34]: 35,40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[35]: 40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[36]: 40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[37]: 40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[38]: 40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[39]: 40,45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[40]: 45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[41]: 45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[42]: 45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[43]: 45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[44]: 45,50,55,60,65,70,75,80,85,90,95,\n",
      "\t[45]: 50,55,60,65,70,75,80,85,90,95,\n",
      "\t[46]: 50,55,60,65,70,75,80,85,90,95,\n",
      "\t[47]: 50,55,60,65,70,75,80,85,90,95,\n",
      "\t[48]: 50,55,60,65,70,75,80,85,90,95,\n",
      "\t[49]: 50,55,60,65,70,75,80,85,90,95,\n",
      "\t[50]: 55,60,65,70,75,80,85,90,95,\n",
      "\t[51]: 55,60,65,70,75,80,85,90,95,\n",
      "\t[52]: 55,60,65,70,75,80,85,90,95,\n",
      "\t[53]: 55,60,65,70,75,80,85,90,95,\n",
      "\t[54]: 55,60,65,70,75,80,85,90,95,\n",
      "\t[55]: 60,65,70,75,80,85,90,95,\n",
      "\t[56]: 60,65,70,75,80,85,90,95,\n",
      "\t[57]: 60,65,70,75,80,85,90,95,\n",
      "\t[58]: 60,65,70,75,80,85,90,95,\n",
      "\t[59]: 60,65,70,75,80,85,90,95,\n",
      "\t[60]: 65,70,75,80,85,90,95,\n",
      "\t[61]: 65,70,75,80,85,90,95,\n",
      "\t[62]: 65,70,75,80,85,90,95,\n",
      "\t[63]: 65,70,75,80,85,90,95,\n",
      "\t[64]: 65,70,75,80,85,90,95,\n",
      "\t[65]: 70,75,80,85,90,95,\n",
      "\t[66]: 70,75,80,85,90,95,\n",
      "\t[67]: 70,75,80,85,90,95,\n",
      "\t[68]: 70,75,80,85,90,95,\n",
      "\t[69]: 70,75,80,85,90,95,\n",
      "\t[70]: 75,80,85,90,95,\n",
      "\t[71]: 75,80,85,90,95,\n",
      "\t[72]: 75,80,85,90,95,\n",
      "\t[73]: 75,80,85,90,95,\n",
      "\t[74]: 75,80,85,90,95,\n",
      "\t[75]: 80,85,90,95,\n",
      "\t[76]: 80,85,90,95,\n",
      "\t[77]: 80,85,90,95,\n",
      "\t[78]: 80,85,90,95,\n",
      "\t[79]: 80,85,90,95,\n",
      "\t[80]: 85,90,95,\n",
      "\t[81]: 85,90,95,\n",
      "\t[82]: 85,90,95,\n",
      "\t[83]: 85,90,95,\n",
      "\t[84]: 85,90,95,\n",
      "\t[85]: 90,95,\n",
      "\t[86]: 90,95,\n",
      "\t[87]: 90,95,\n",
      "\t[88]: 90,95,\n",
      "\t[89]: 90,95,\n",
      "\t[90]: 95,\n",
      "\t[91]: 95,\n",
      "\t[92]: 95,\n",
      "\t[93]: 95,\n",
      "\t[94]: 95,\n",
      "\t[95]: \n",
      "\t[96]: \n",
      "\t[97]: \n",
      "\t[98]: \n",
      "\t[99]: \n",
      "Layer 1: completed\n",
      "\n",
      "[Octagon abstraction] Processing layer fc2\n",
      "\t[0]: 5,10,15,20,25,30,35,40,45,\n",
      "\t[1]: 5,10,15,20,25,30,35,40,45,\n",
      "\t[2]: 5,10,15,20,25,30,35,40,45,\n",
      "\t[3]: 5,10,15,20,25,30,35,40,45,\n",
      "\t[4]: 5,10,15,20,25,30,35,40,45,\n",
      "\t[5]: 10,15,20,25,30,35,40,45,\n",
      "\t[6]: 10,15,20,25,30,35,40,45,\n",
      "\t[7]: 10,15,20,25,30,35,40,45,\n",
      "\t[8]: 10,15,20,25,30,35,40,45,\n",
      "\t[9]: 10,15,20,25,30,35,40,45,\n",
      "\t[10]: 15,20,25,30,35,40,45,\n",
      "\t[11]: 15,20,25,30,35,40,45,\n",
      "\t[12]: 15,20,25,30,35,40,45,\n",
      "\t[13]: 15,20,25,30,35,40,45,\n",
      "\t[14]: 15,20,25,30,35,40,45,\n",
      "\t[15]: 20,25,30,35,40,45,\n",
      "\t[16]: 20,25,30,35,40,45,\n",
      "\t[17]: 20,25,30,35,40,45,\n",
      "\t[18]: 20,25,30,35,40,45,\n",
      "\t[19]: 20,25,30,35,40,45,\n",
      "\t[20]: 25,30,35,40,45,\n",
      "\t[21]: 25,30,35,40,45,\n",
      "\t[22]: 25,30,35,40,45,\n",
      "\t[23]: 25,30,35,40,45,\n",
      "\t[24]: 25,30,35,40,45,\n",
      "\t[25]: 30,35,40,45,\n",
      "\t[26]: 30,35,40,45,\n",
      "\t[27]: 30,35,40,45,\n",
      "\t[28]: 30,35,40,45,\n",
      "\t[29]: 30,35,40,45,\n",
      "\t[30]: 35,40,45,\n",
      "\t[31]: 35,40,45,\n",
      "\t[32]: 35,40,45,\n",
      "\t[33]: 35,40,45,\n",
      "\t[34]: 35,40,45,\n",
      "\t[35]: 40,45,\n",
      "\t[36]: 40,45,\n",
      "\t[37]: 40,45,\n",
      "\t[38]: 40,45,\n",
      "\t[39]: 40,45,\n",
      "\t[40]: 45,\n",
      "\t[41]: 45,\n",
      "\t[42]: 45,\n",
      "\t[43]: 45,\n",
      "\t[44]: 45,\n",
      "\t[45]: \n",
      "\t[46]: \n",
      "\t[47]: \n",
      "\t[48]: \n",
      "\t[49]: \n",
      "Layer 2: completed\n",
      "\n",
      "[Octagon abstraction] Processing layer fc3\n",
      "Layer 3: completed\n",
      "\n",
      "3  0\n",
      "3  1\n",
      "3  2\n",
      "3  3\n",
      "3  4\n",
      "3  5\n",
      "3  6\n",
      "3  7\n",
      "3  8\n",
      "3  9\n",
      "3  10\n",
      "3  11\n",
      "3  12\n",
      "3  13\n",
      "3  14\n",
      "3  15\n",
      "3  16\n",
      "3  17\n",
      "3  18\n",
      "3  19\n",
      "3  20\n"
     ]
    }
   ],
   "source": [
    "# The last parameter is to enable octagon abstraction\n",
    "# The last parameter is set for enabling octagon abstraction\n",
    "minBoundOctagon, maxBoundOctagon = staticanalysis.verify(inputMinBound, inputMaxBound, net, False, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Output bound based on octagon abstraction =====\n",
      "Min:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Max:\n",
      "[313.10431    300.56357    245.09364    309.24808    219.18987\n",
      " 279.08622    265.27005    170.09941    183.04725    160.12929\n",
      " 169.68391    169.87759    180.43413    128.6642     138.26772\n",
      "  89.270456     0.43533164   0.           0.           0.\n",
      " 195.16936   ]\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Output bound based on octagon abstraction =====\")\n",
    "print(\"Min:\")\n",
    "print(minBoundOctagon)\n",
    "print(\"Max:\")\n",
    "print(maxBoundOctagon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that using Octagon abstraction domain, the output bound is indeed more precise (every value in the octagon abstraction is smaller than boxed abstraction), but the margin is not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
