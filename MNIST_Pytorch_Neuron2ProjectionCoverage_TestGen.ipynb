{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron 2-projection on-off activation coverage and test gen (1) - MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by fixing the random seed to ensure reproducability. In all our examples, we use 42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "# Fix the number for repeatability (we have also stored the trained model)\n",
    "numpy.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required library, and set the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.misc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd  import Variable\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.misc import imsave\n",
    "import os\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyper-parameters. The parameter \"sizeOfNeuronsToMonitor\" is the number of neurons we will monitor, which is the 2nd to last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "sizeOfNeuronsToMonitor = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then , prepare the training and test set. MNIST can be directly downloaded. Notice that we do not normalize the input data, but just rely on small learning rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data/mnist', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data/mnist', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to display MNIST images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "util.displayMNIST(images[0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the network \n",
    "\n",
    "To use run-time monitoring, apart from standard \"forward()\" function, we additionally define another function to return values of intermediate layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    " \n",
    "        self.conv1 = nn.Conv2d(1, 40, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(40, 20, 5)\n",
    "        self.fc1 = nn.Linear(20 * 4 * 4, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80, sizeOfNeuronsToMonitor)\n",
    "        self.fc4 = nn.Linear(sizeOfNeuronsToMonitor, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original 28x28x1 -(conv)-> 24x24x40 -(pool)-> 12x12x40\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Original 12x12x40 -(conv)-> 8x8x20 -(pool)-> 4x4x20\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten it to an array of inputs\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out \n",
    "  \n",
    "    # Here we add another function, which does the same forward computation but also extracts intermediate layer results\n",
    "    def forwardWithIntermediate(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 20 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        intermediateValues = x\n",
    "        x = F.relu(x)\n",
    "        out = self.fc4(x)\n",
    "        return out, intermediateValues    \n",
    "    \n",
    "net = NeuralNet()\n",
    "net.eval()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model (if you have a pretrained one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model to be attacked\n",
    "# net = NeuralNet()\n",
    "net.load_state_dict(torch.load('models/1_model_MNIST_CNN.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "furtherTrain = False\n",
    "\n",
    "if furtherTrain: \n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.81 %\n"
     ]
    }
   ],
   "source": [
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    outofActivationPattern = 0\n",
    "    outofActivationPatternAndResultWrong = 0\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger 2-projection neuron on-off activation coverage computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.metrics import KProjection\n",
    "\n",
    "k_Value = 2\n",
    "\n",
    "metric = KProjection.Neuron_OnOff_KProjection_Metric(k_Value, sizeOfNeuronsToMonitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger the function addInputs() to update the k-projection table based on all visited patterns for each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current input size fed into the metric: 64\n",
      "2-projection neuron on-off activation coverage:2780/3120=0.8910256410256411\n",
      "\n",
      "\n",
      "Current input size fed into the metric: 3264\n",
      "2-projection neuron on-off activation coverage:2909/3120=0.9323717948717949\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7122784bd389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintermediateValues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardWithIntermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Add the batch of neuron activation patterns to the k-projection table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8290cd8be46d>\u001b[0m in \u001b[0;36mforwardWithIntermediate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Here we add another function, which does the same forward computation but also extracts intermediate layer results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforwardWithIntermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        total = total + (len(labels))\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        \n",
    "        # Add the batch of neuron activation patterns to the k-projection table\n",
    "        metric.addInputs(intermediateValues.numpy())\n",
    "                \n",
    "        if(i % 50) == 0:\n",
    "            print('Current input size fed into the metric: '+str(total))\n",
    "            metric.printMetricQuantity()\n",
    "            print(\"\\n\")\n",
    "        i = i+1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-projection neuron on-off activation coverage:2912/3120=0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "metric.printMetricQuantity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Now, ask the test case generator to derive us a pattern which maximally increases 2-projection coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.atg.nap import napgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum possibility for improvement = 208\n",
      "Optimal objective value computed from IP = 126\n",
      "\n",
      "for neuron 0, set it to 1\n",
      "for neuron 1, set it to 1\n",
      "for neuron 2, set it to 0\n",
      "for neuron 3, set it to 1\n",
      "for neuron 4, set it to 1\n",
      "for neuron 5, set it to 0\n",
      "for neuron 6, set it to 1\n",
      "for neuron 7, set it to 0\n",
      "for neuron 8, set it to 0\n",
      "for neuron 9, set it to 0\n",
      "for neuron 10, set it to 0\n",
      "for neuron 11, set it to 1\n",
      "for neuron 12, set it to 0\n",
      "for neuron 13, set it to 1\n",
      "for neuron 14, set it to 1\n",
      "for neuron 15, set it to 0\n",
      "for neuron 16, set it to 0\n",
      "for neuron 17, set it to 1\n",
      "for neuron 18, set it to 1\n",
      "for neuron 19, set it to 0\n",
      "for neuron 20, set it to 0\n",
      "for neuron 21, set it to 0\n",
      "for neuron 22, set it to 1\n",
      "for neuron 23, set it to 0\n",
      "for neuron 24, set it to 1\n",
      "for neuron 25, set it to 1\n",
      "for neuron 26, set it to 0\n",
      "for neuron 27, set it to 0\n",
      "for neuron 28, set it to 0\n",
      "for neuron 29, set it to 1\n",
      "for neuron 30, set it to 0\n",
      "for neuron 31, set it to 1\n",
      "for neuron 32, set it to 1\n",
      "for neuron 33, set it to 0\n",
      "for neuron 34, set it to 1\n",
      "for neuron 35, set it to 0\n",
      "for neuron 36, set it to 0\n",
      "for neuron 37, set it to 1\n",
      "for neuron 38, set it to 1\n",
      "for neuron 39, set it to 1\n"
     ]
    }
   ],
   "source": [
    "napgen.proposeNAPcandidate(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pick an image, and try to find a perturbation to satisfy a specific neuron pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "util.displayMNIST(images[0].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to contril neuron 3 and neuron 4 such that there sign is positive (activated) and negative (deactivated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.atg.gradient import gratestgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-12.7952,   6.6679,   8.4289,  13.3291,   5.2758,  -6.2810,\n",
      "           7.5812,   7.0786,   5.5022,   2.9232,  12.3116,  -8.2617,\n",
      "           1.4571,  -1.9588, -11.2161,  -1.1910,   5.8543,  -5.8700,\n",
      "           0.0959,  -4.7724,   8.2621,  -1.7447,   8.1505,   5.3886,\n",
      "          -3.7132,  -5.8331,  -5.2370,  10.9167,   9.2767,  -9.3469,\n",
      "          10.3210,  -5.9824,  -4.1280,  -1.7606,  -4.2216,  12.2566,\n",
      "           5.1342,  10.6252,  -6.2162,  -9.4020]])\n",
      "tensor([[-12.7952,   6.6679,   8.4289,  13.3291,  -1.0000,  -6.2810,\n",
      "           7.5812,   7.0786,   5.5022,   2.9232,  12.3116,  -8.2617,\n",
      "           1.4571,  -1.9588, -11.2161,  -1.1910,   5.8543,  -5.8700,\n",
      "           0.0959,  -4.7724,   8.2621,  -1.7447,   8.1505,   5.3886,\n",
      "          -3.7132,  -5.8331,  -5.2370,  10.9167,   9.2767,  -9.3469,\n",
      "          10.3210,  -5.9824,  -4.1280,  -1.7606,  -4.2216,  12.2566,\n",
      "           5.1342,  10.6252,  -6.2162,  -9.4020]])\n",
      "0: 13.329125, 5.2757874\n",
      "500: 0.20578521, 0.113090836\n",
      "Found an image to successfully create the required pattern:\n"
     ]
    }
   ],
   "source": [
    "targetedNeuronIndex= list()\n",
    "desiredNAP = list()\n",
    "\n",
    "# Specify the target - we want to contril neuron 3 and neuron 4, with sign being positiove and negative\n",
    "targetedNeuronIndex.append(3)\n",
    "targetedNeuronIndex.append(4)\n",
    "\n",
    "desiredNAP.append(1)\n",
    "desiredNAP.append(-1)\n",
    "\n",
    "# Trigger gradient-based test case generation\n",
    "new_image, testcaseGenSuccessful = gratestgen.generateTestCase(images[0].unsqueeze(0), targetedNeuronIndex, desiredNAP, net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron activation: 0.11034143, -4.3325126e-05\n"
     ]
    }
   ],
   "source": [
    "if testcaseGenSuccessful: \n",
    "    util.displayMNIST(new_image[0].numpy())\n",
    "    out, intermediate = net.forwardWithIntermediate(new_image)\n",
    "    print(\"neuron activation: \"+str(intermediate.detach().numpy().squeeze(0)[targetedNeuronIndex[0]]) + \", \"+ str(intermediate.detach().numpy().squeeze(0)[targetedNeuronIndex[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if testcaseGenSuccessful: \n",
    "    util.displayMNIST((new_image[0].numpy() - images[0].numpy()) + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
