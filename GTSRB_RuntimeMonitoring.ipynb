{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Monitoring Neuron Activation Patterns (2) - German Traffic Sign Recognition Benchmark (GTSRB)\n",
    "\n",
    "For detailed explanation, please refer to the jupyter notebook 1_MNIST_Pytorch_CNN.ipydb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random\n",
    "# Fix the number for repeatability (we have also stored the trained model)\n",
    "numpy.random.seed(42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by accessing the root folder where internally, subfolders are images with folder name being their classified result.\n",
    "\n",
    "The dataset should be available at the following site \n",
    "http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset\n",
    "\n",
    "Here we just pick the 26K smaller training set (the online version), as the training and test set can all be directly loaded using PyTorch included functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "standard_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # Change the image to PIL format, such that resize can be done\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((32,32)),\n",
    "        # Bring it back to tensor\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# Here the data is not be normalized to [-1,1]\n",
    "\n",
    "# Change the folder based on your specific needs. \n",
    "# This one is a smaller (26640 examples) data set (for online training), so the \n",
    "data = ImageFolder(root='data/GTSRB-Training_fixed/GTSRB/Training',  transform=standard_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000', '00001', '00002', '00003', '00004', '00005', '00006', '00007', '00008', '00009', '00010', '00011', '00012', '00013', '00014', '00015', '00016', '00017', '00018', '00019', '00020', '00021', '00022', '00023', '00024', '00025', '00026', '00027', '00028', '00029', '00030', '00031', '00032', '00033', '00034', '00035', '00036', '00037', '00038', '00039', '00040', '00041', '00042']\n"
     ]
    }
   ],
   "source": [
    "print(data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 \t Speed limit (20km/h)\n",
    "- 1 \t Speed limit (30km/h)\n",
    "- 2 \t Speed limit (50km/h)\n",
    "- 3 \t Speed limit (60km/h)\n",
    "- 4 \t Speed limit (70km/h)\n",
    "- 5 \t Speed limit (80km/h)\n",
    "- 6 \t End of speed limit (80km/h)\n",
    "- 7 \t Speed limit (100km/h)\n",
    "- 8 \t Speed limit (120km/h)\n",
    "- 9 \t No passing\n",
    "- 10 \t No passing for vechiles over 3.5 metric tons\n",
    "- 11 \t Right-of-way at the next intersection\n",
    "- 12 \t Priority road\n",
    "- 13 \t Yield\n",
    "- 14 \t Stop\n",
    "- 15 \t No vechiles\n",
    "- 16 \t Vechiles over 3.5 metric tons prohibited\n",
    "- 17 \t No entry\n",
    "- 18 \t General caution\n",
    "- 19 \t Dangerous curve to the left\n",
    "- 20 \t Dangerous curve to the right\n",
    "- 21 Double curve\n",
    "- 22 \t Bumpy road\n",
    "- 23 \t Slippery road\n",
    "- 24 \t Road narrows on the right\n",
    "- 25 \t Road work\n",
    "- 26 \t Traffic signals\n",
    "- 27 \t Pedestrians\n",
    "- 28 \t Children crossing\n",
    "- 29 \t Bicycles crossing\n",
    "- 30 \t Beware of ice/snow\n",
    "- 31 \t Wild animals crossing\n",
    "- 32 \t End of all speed and passing limits\n",
    "- 33 \t Turn right ahead\n",
    "- 34 \t Turn left ahead\n",
    "- 35 \t Ahead only\n",
    "- 36 \t Go straight or right\n",
    "- 37 \t Go straight or left\n",
    "- 38 \t Keep right\n",
    "- 39 \t Keep left\n",
    "- 40 \t Roundabout mandatory\n",
    "- 41 \t End of no passing\n",
    "- 42 \t End of no passing by vechiles over 3.5 metric tons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_classes = 43\n",
    "learning_rate = 0.001\n",
    "sizeOfNeuronsToMonitor = 84\n",
    "batch_size = 64\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print(images.shape)\n",
    "util.displayGTSRB(images[0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 40, 5)\n",
    "        self.conv1_bn = nn.BatchNorm2d(40)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(40, 20, 5)\n",
    "        self.conv2_bn = nn.BatchNorm2d(20)\n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 240)        \n",
    "        self.fc2 = nn.Linear(240, sizeOfNeuronsToMonitor)\n",
    "        self.fc3 = nn.Linear(sizeOfNeuronsToMonitor, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2_bn((self.conv2(x)))))\n",
    "        # Flatten it to an array of inputs\n",
    "        x = x.view(-1, 20 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "\n",
    "    def forwardWithIntermediate(self, x):\n",
    "        x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2_bn((self.conv2(x)))))\n",
    "        # Flatten it to an array of inputs\n",
    "        x = x.view(-1, 20 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        intermediateValues = F.relu(self.fc2(x))\n",
    "        x = self.fc3(intermediateValues)\n",
    "        return x , intermediateValues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. to load pre-trained model:\n",
    "net.load_state_dict(torch.load('models/3_model_GTSRB_CNN_27k_train99%.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "furtherTrain = False\n",
    "\n",
    "if furtherTrain: \n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loader):  \n",
    "            # Move tensors to the configured device\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, we should also use the test data, where they are separated into folders based on their classes, followed by shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "testdata = ImageFolder(root='data/GTSRB_Online-Test-Images-Sorted/GTSRB/Online-Test-sort', transform=standard_transform)\n",
    "testloader = DataLoader(testdata, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "util.displayGTSRB(images[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 10])\n",
      "tensor([ 10])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(predicted)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network over test images: 96.39589466146869 %\n",
      "\n",
      "\n",
      "Accuracy of     0 : 88 %\n",
      "Accuracy of     1 : 98 %\n",
      "Accuracy of     2 : 98 %\n",
      "Accuracy of     3 : 98 %\n",
      "Accuracy of     4 : 95 %\n",
      "Accuracy of     5 : 92 %\n",
      "Accuracy of     6 : 97 %\n",
      "Accuracy of     7 : 98 %\n",
      "Accuracy of     8 : 96 %\n",
      "Accuracy of     9 : 98 %\n",
      "Accuracy of    10 : 98 %\n",
      "Accuracy of    11 : 95 %\n",
      "Accuracy of    12 : 99 %\n",
      "Accuracy of    13 : 99 %\n",
      "Accuracy of    14 : 96 %\n",
      "Accuracy of    15 : 93 %\n",
      "Accuracy of    16 : 100 %\n",
      "Accuracy of    17 : 99 %\n",
      "Accuracy of    18 : 97 %\n",
      "Accuracy of    19 : 100 %\n",
      "Accuracy of    20 : 99 %\n",
      "Accuracy of    21 : 37 %\n",
      "Accuracy of    22 : 98 %\n",
      "Accuracy of    23 : 87 %\n",
      "Accuracy of    24 : 94 %\n",
      "Accuracy of    25 : 99 %\n",
      "Accuracy of    26 : 91 %\n",
      "Accuracy of    27 : 100 %\n",
      "Accuracy of    28 : 91 %\n",
      "Accuracy of    29 : 62 %\n",
      "Accuracy of    30 : 92 %\n",
      "Accuracy of    31 : 96 %\n",
      "Accuracy of    32 : 100 %\n",
      "Accuracy of    33 : 95 %\n",
      "Accuracy of    34 : 100 %\n",
      "Accuracy of    35 : 99 %\n",
      "Accuracy of    36 : 100 %\n",
      "Accuracy of    37 : 100 %\n",
      "Accuracy of    38 : 92 %\n",
      "Accuracy of    39 : 96 %\n",
      "Accuracy of    40 : 90 %\n",
      "Accuracy of    41 : 100 %\n",
      "Accuracy of    42 : 100 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels)\n",
    "        label = labels[0]\n",
    "        class_correct[label] += c[0].item()\n",
    "        class_total[label] += 1\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network over test images: {} %\\n\\n'.format(100 * correct / total))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        i, 100 * class_correct[i] / class_total[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network over train images: 99.92867867867868 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network over train images: {} %\\n\\n'.format(100 * correct / total))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider monitoring the stop sign class, where apart from monitoring the complete neurons in fc3 (a total of 84 neurons), we take those whose impact (represented by the outgoing weight - in this case, the weight is the partial derivitative) is higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc3.weight tensor([-0.8384, -0.6619, -0.6385, -0.3713,  0.0685, -0.9585, -0.1548,\n",
      "        -0.8818, -0.7139, -0.9509,  0.1551, -0.5067, -1.6361, -0.0824,\n",
      "        -0.6102, -0.9059, -0.0051, -0.9088, -0.1104, -1.0027, -0.4363,\n",
      "         0.0878, -0.1808, -0.6895, -0.5737,  0.1477, -0.0297, -0.4882,\n",
      "        -0.0085,  0.0617, -0.0161, -0.3357, -0.0453, -0.0137,  0.0956,\n",
      "        -0.8550, -0.1750, -0.5308, -0.5634, -0.1976, -1.1292, -0.1395,\n",
      "        -0.9964, -0.2545, -0.7173,  0.0244, -0.9996, -0.0917,  0.0716,\n",
      "        -1.2760, -0.2818, -0.3805, -0.6418, -1.0037, -1.3294, -0.0093,\n",
      "         0.1419, -0.0160,  0.0150, -0.5254, -0.9061, -0.2340, -1.5008,\n",
      "        -0.1813,  0.1064, -0.1602, -0.2963, -0.6089, -0.7957,  0.1285,\n",
      "        -0.0941, -0.0856, -0.0253, -0.5752, -0.6543, -0.4661, -0.8133,\n",
      "        -0.5880, -0.7916, -1.1879, -0.6502,  0.1297, -0.7753, -0.5738])\n",
      "neurons omitted for monitoring: 62\n"
     ]
    }
   ],
   "source": [
    "stopSignClass = 14\n",
    "\n",
    "weightsStopSignClass = None\n",
    "neuronIndicesToBeOmitted = {}\n",
    "\n",
    "# As a parameter to derive the number of neurons to be tracked\n",
    "filteringRate = 0.45\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if name == \"fc3.weight\":\n",
    "        print(name, param.data[stopSignClass])\n",
    "        weightsStopSignClass = param.data[stopSignClass].numpy()\n",
    "        \n",
    "absWeight = np.absolute(weightsStopSignClass)\n",
    "maxWeightQuantity = np.max(absWeight)\n",
    "\n",
    "neuronIndicesToBeOmittedByStopSign = set()\n",
    "\n",
    "for i in range(len(absWeight)):\n",
    "    if(absWeight[i] <= filteringRate * maxWeightQuantity):\n",
    "        neuronIndicesToBeOmittedByStopSign.add(i)\n",
    "\n",
    "# print(neuronIndicesToBeOmittedByStopSign)\n",
    "\n",
    "neuronIndicesToBeOmitted[stopSignClass] = neuronIndicesToBeOmittedByStopSign\n",
    "print(\"neurons omitted for monitoring: \"+str(len(neuronIndicesToBeOmitted[stopSignClass])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Trigger neuron on-off activation pattern monitoring\n",
    "\n",
    "Here we pretend that test set is the \"real data\" after deployment, so we only record neuron activation patterns inside the training set. \n",
    "\n",
    "Notice: The BDD can consume a lot memory (>12GB)! This is because we do not apply any variable re-ordering techniques to reduce the size. Before actual deployment for run-time monitoring, one should perform proper reordering.\n",
    "\n",
    "Therefore, here we restrict ourselves to the classification of stop sign, so the monitor size can be smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nndependability.rv import napmonitor\n",
    "monitor = napmonitor.NAP_Monitor(num_classes, sizeOfNeuronsToMonitor, neuronIndicesToBeOmitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the all train images: 99.93243243243244 %\n"
     ]
    }
   ],
   "source": [
    "## Scan the training set and store activation pattern on BDD\n",
    "\n",
    "stopSignClass = 14\n",
    "\n",
    "testOnAllClasses = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if testOnAllClasses:\n",
    "            # Add the batch of neuron activation patterns to the monitor, for Hamming distance = 0, and for all classes. \n",
    "            monitor.addAllNeuronPatternsToClass(intermediateValues.numpy(), predicted.numpy(), labels.numpy(), -1)\n",
    "        else:\n",
    "            monitor.addAllNeuronPatternsToClass(intermediateValues.numpy(), predicted.numpy(), labels.numpy(), stopSignClass)            \n",
    "\n",
    "     \n",
    "    print('Accuracy of the network on the all train images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "def testMonitor(testOnAllClasses, loader, monitor):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        total = 0\n",
    "        correct = 0\n",
    "        correctStopSign = 0\n",
    "        outofActivationPattern = 0\n",
    "        outofActivationPatternAndResultWrong = 0\n",
    "\n",
    "        totalStopSign = 0\n",
    "        for images, labels in loader:\n",
    "            labels = labels.to(device)\n",
    "            outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "            # Additional processing for runtime monitoring\n",
    "            predictedNp = predicted.numpy()\n",
    "\n",
    "            result = (predicted == labels)\n",
    "            res = result.numpy()\n",
    "\n",
    "            # Iterate over each image in the batch\n",
    "            for exampleIndex in range(intermediateValues.shape[0]):   \n",
    "                # Check if the prediction says that it is a stop sign\n",
    "                if testOnAllClasses or ((not testOnAllClasses) and predicted.numpy()[exampleIndex] == stopSignClass):\n",
    "                    if not monitor.isPatternContained(intermediateValues.numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                        outofActivationPattern = outofActivationPattern +1\n",
    "                        if res[exampleIndex] == False :\n",
    "                            outofActivationPatternAndResultWrong = outofActivationPatternAndResultWrong+ 1\n",
    "            \n",
    "    if testOnAllClasses:\n",
    "        print('Accuracy of the network on the GTSRB test images: {} %'.format(100 * correct / total))\n",
    "        print('Out-of-activation pattern on the GTSRB test images: {} %'.format(100 * outofActivationPattern / total))\n",
    "        print('Out-of-activation pattern & misclassified / out-of-activation pattern : {} %'.format(100 * outofActivationPatternAndResultWrong / (total - correct)))\n",
    "\n",
    "    else: \n",
    "        print('Accuracy of the network on total GTSRB test images, for stop sign: {} %'.format(100 * class_correct[stopSignClass] / class_total[stopSignClass]))\n",
    "        print('Out-of-extended activation pattern on GTSRB test images, for stop sign: {} %'.format(100 * outofActivationPattern / class_total[stopSignClass]))\n",
    "        print('Out-of-extended activation pattern & misclassified / out-of-extended activation pattern (stop sign) : {} %'.format(100 * outofActivationPatternAndResultWrong / outofActivationPattern))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on total GTSRB test images, for stop sign: 96.66666666666667 %\n",
      "Out-of-extended activation pattern on GTSRB test images, for stop sign: 32.916666666666664 %\n",
      "Out-of-extended activation pattern & misclassified / out-of-extended activation pattern (stop sign) : 10.126582278481013 %\n"
     ]
    }
   ],
   "source": [
    "testMonitor(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce adversarial training example using iterative FGSM and check if the activation pattern is within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util \n",
    "\n",
    "\n",
    "def performAdversarialExamination(testOnAllClasses, loader, monitor):\n",
    "    \n",
    "    originalImages = []\n",
    "    originalLabels = []\n",
    "    attackedImages = []\n",
    "\n",
    "    notInNeuronActivationPattern = 0\n",
    "    totalPerturbed = 0\n",
    "\n",
    "    # Note: Only perform analysis when the monitor takes all classes\n",
    "    if testOnAllClasses:\n",
    "\n",
    "        for images, labels in loader:\n",
    "            # print(list(images.size())[0])\n",
    "\n",
    "            for i in range(list(images.size())[0]):\n",
    "                if testOnAllClasses:\n",
    "                    adv_img, noise, attackSuccessful = util.iterative_FGSM_attack(images[i].unsqueeze(0), labels[i].unsqueeze(0), net)\n",
    "\n",
    "                    if (attackSuccessful) :\n",
    "                        totalPerturbed = totalPerturbed + 1\n",
    "\n",
    "                        out, intermediateValues = net.forwardWithIntermediate(adv_img)  \n",
    "                        _, predicted = torch.max(out.data, 1)\n",
    "                        iv = intermediateValues.detach().numpy()\n",
    "                        mat = np.zeros(intermediateValues.shape)\n",
    "                        ivabs = np.greater(iv, mat)\n",
    "\n",
    "                        for exampleIndex in range(iv.shape[0]): \n",
    "                            if not monitor.isOnOffPatternContained(ivabs[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                                notInNeuronActivationPattern = notInNeuronActivationPattern +1\n",
    "\n",
    "\n",
    "            print('not in neuron activation pattern / all total perturbed: {} %'.format(100 * notInNeuronActivationPattern / totalPerturbed))\n",
    "    else:\n",
    "        # Try to perturb, hopefully perturbed to stop sign\n",
    "        targetedPerturbedClass = torch.ones(1, dtype=torch.long)\n",
    "        targetedPerturbedClass[0] = stopSignClass\n",
    "\n",
    "        for images, labels in loader:\n",
    "            # print(list(images.size())[0])\n",
    "\n",
    "            for i in range(list(images.size())[0]):\n",
    "                if(testOnAllClasses or ((not testOnAllClasses) and labels[i].unsqueeze(0) != stopSignClass)):\n",
    "                    # From those image that are not stop sign, and perturb them.\n",
    "                    adv_img, noise, attackSuccessful = util.iterative_targeted_FGSM_attack(images[i].unsqueeze(0), labels[i].unsqueeze(0), net, targetedPerturbedClass)\n",
    "\n",
    "                    if (attackSuccessful) :\n",
    "                        totalPerturbed = totalPerturbed + 1\n",
    "\n",
    "                        if(totalPerturbed % 100 == 0):\n",
    "                            print(totalPerturbed)\n",
    "                            originalImages.append(images[i])\n",
    "                            originalLabels.append(labels[i])\n",
    "                            attackedImages.append(adv_img)\n",
    "\n",
    "                        if(totalPerturbed == 2000):\n",
    "                            # In the worst case, we only perturn 5000 images\n",
    "                            break\n",
    "\n",
    "                        # util.displayGTSRB(adv_img[0].numpy())\n",
    "                        out, intermediateValues = net.forwardWithIntermediate(adv_img)  \n",
    "                        _, predicted = torch.max(out.data, 1)\n",
    "                        iv = intermediateValues.detach().numpy()\n",
    "                        mat = np.zeros(intermediateValues.shape)\n",
    "                        ivabs = np.greater(iv, mat)\n",
    "\n",
    "                        for exampleIndex in range(iv.shape[0]):                             \n",
    "                            if not monitor.isOnOffPatternContained(ivabs[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                                notInNeuronActivationPattern = notInNeuronActivationPattern +1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if(totalPerturbed>0):\n",
    "        print('not in neuron activation pattern / all total perturbed: {} %'.format(100 * notInNeuronActivationPattern / totalPerturbed))\n",
    "        print('all total perturbed = '+str(totalPerturbed))\n",
    "    else:\n",
    "        print('No image can be perturbed to the desired class!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performAdversarialExamination(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.enlargeSetByOneBitFluctuation(stopSignClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on total GTSRB test images, for stop sign: 96.66666666666667 %\n",
      "Out-of-extended activation pattern on GTSRB test images, for stop sign: 13.75 %\n",
      "Out-of-extended activation pattern & misclassified / out-of-extended activation pattern (stop sign) : 21.21212121212121 %\n"
     ]
    }
   ],
   "source": [
    "testMonitor(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performAdversarialExamination(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.enlargeSetByOneBitFluctuation(stopSignClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on total GTSRB test images, for stop sign: 96.66666666666667 %\n",
      "Out-of-extended activation pattern on GTSRB test images, for stop sign: 7.083333333333333 %\n",
      "Out-of-extended activation pattern & misclassified / out-of-extended activation pattern (stop sign) : 41.1764705882353 %\n"
     ]
    }
   ],
   "source": [
    "testMonitor(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performAdversarialExamination(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.enlargeSetByOneBitFluctuation(stopSignClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on total GTSRB test images, for stop sign: 96.66666666666667 %\n",
      "Out-of-extended activation pattern on GTSRB test images, for stop sign: 4.583333333333333 %\n",
      "Out-of-extended activation pattern & misclassified / out-of-extended activation pattern (stop sign) : 54.54545454545455 %\n"
     ]
    }
   ],
   "source": [
    "testMonitor(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "not in neuron activation pattern / all total perturbed: 10.436953807740325 %\n",
      "all total perturbed = 4005\n"
     ]
    }
   ],
   "source": [
    "performAdversarialExamination(testOnAllClasses, testloader, monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty print some adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(predicted)\n",
    "print(labels)\n",
    "\n",
    "util.displayGTSRB(images[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "img = images[0].unsqueeze(0)\n",
    "label = labels[0].unsqueeze(0)\n",
    "\n",
    "adv_img, noise, attackSuccessful = util.iterative_FGSM_attack(img, label, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attackSuccessful :\n",
    "    util.displayGTSRB(adv_img[0].numpy())\n",
    "else: \n",
    "    print(\"iterative FGSM is not effective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we directly print the noise, we got complaints as the value is not within [0, 1]. Therefore, we add each pixel of RGB with 0.5 to display the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attackSuccessful :\n",
    "    util.displayGTSRB(noise[0].numpy()+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the adversarial image produces any similar activation pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, intermediateValues = net.forwardWithIntermediate(adv_img)        \n",
    "print(out.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(out.data, 1)\n",
    "iv = intermediateValues.detach().numpy()\n",
    "mat = np.zeros(intermediateValues.shape)\n",
    "ivabs = np.greater(iv, mat)\n",
    "\n",
    "# Note: Only perform analysis when the monitor takes all classes\n",
    "if testOnAllClasses:\n",
    "    for exampleIndex in range(iv.shape[0]):      \n",
    "        if not monitor.isPatternContained(intermediateValues.detach().numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "            print(\"Adv example is not within activation pattern\")\n",
    "        else:\n",
    "            print(\"Adv example is within activation pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (Optional). Trigger neuron interval activation pattern monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopSignClass = 14\n",
    "\n",
    "weightsStopSignClass = None\n",
    "neuronIndicesToBeOmitted = {}\n",
    "\n",
    "# As a parameter to derive the number of neurons to be tracked\n",
    "filteringRate = 0.3\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if name == \"fc3.weight\":\n",
    "        print(name, param.data[stopSignClass])\n",
    "        weightsStopSignClass = param.data[stopSignClass].numpy()\n",
    "        \n",
    "absWeight = np.absolute(weightsStopSignClass)\n",
    "maxWeightQuantity = np.max(absWeight)\n",
    "\n",
    "neuronIndicesToBeOmittedByStopSign = set()\n",
    "\n",
    "for i in range(len(absWeight)):\n",
    "    if(absWeight[i] <= filteringRate * maxWeightQuantity):\n",
    "        neuronIndicesToBeOmittedByStopSign.add(i)\n",
    "\n",
    "# print(neuronIndicesToBeOmittedByStopSign)\n",
    "\n",
    "neuronIndicesToBeOmitted[stopSignClass] = neuronIndicesToBeOmittedByStopSign\n",
    "print(\"neurons omitted for monitoring: \"+str(len(neuronIndicesToBeOmitted[stopSignClass])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximumValues = np.zeros([1, sizeOfNeuronsToMonitor])\n",
    "\n",
    "# We don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        labels = labels.to(device)\n",
    "        outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "    \n",
    "    for exampleIndex in range(intermediateValues.shape[0]):   \n",
    "        maximumValues = np.maximum(intermediateValues.numpy()[exampleIndex,:], maximumValues)\n",
    "    \n",
    "    print(maximumValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = (maximumValues*1)/3\n",
    "threshold2 = (maximumValues*2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runIntervalMonitor = True\n",
    "\n",
    "from nndependability.rv import nipmonitor\n",
    "intervalmonitor = nipmonitor.NIP_Monitor(num_classes, sizeOfNeuronsToMonitor, threshold1, threshold2, neuronIndicesToBeOmitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classToAnalyze = stopSignClass\n",
    "# Use below if we want to analyze the complete class\n",
    "# classToAnalyze = -1\n",
    "\n",
    "if runIntervalMonitor: \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            labels = labels.to(device)\n",
    "            outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Add the batch of neuron activation patterns to the monitor, with only the 0-th class\n",
    "            intervalmonitor.addAllNeuronPatternsToClass(intermediateValues.numpy(), predicted.numpy(), labels.numpy(), classToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testIntervalMonitor(testOnAllClasses, loader, monitor):\n",
    "    # We don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        outofActivationPattern = 0\n",
    "        outofActivationPatternAndResultWrong = 0\n",
    "\n",
    "        total = 0\n",
    "        for images, labels in testloader:\n",
    "            labels = labels.to(device)\n",
    "            outputs, intermediateValues = net.forwardWithIntermediate(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Additional processing for runtime monitoring\n",
    "\n",
    "            predictedNp = predicted.numpy()\n",
    "\n",
    "            result = (predicted == labels)\n",
    "            res = result.numpy()\n",
    "\n",
    "            # Iterate over each image in the batch\n",
    "            for exampleIndex in range(intermediateValues.shape[0]): \n",
    "                if (classToAnalyze == -1) or (predicted.numpy()[exampleIndex] == classToAnalyze): \n",
    "                    if not intervalmonitor.isPatternContained(intermediateValues.numpy()[exampleIndex,:], predicted.numpy()[exampleIndex]):\n",
    "                        outofActivationPattern = outofActivationPattern +1\n",
    "                        if res[exampleIndex] == False :\n",
    "                            outofActivationPatternAndResultWrong = outofActivationPatternAndResultWrong + 1\n",
    "\n",
    "        print('Result on interval-based monitor ([0,1,2,3]:= [<=0, 0-33%, 33%-66%, >66%] in max training value: ')      \n",
    "        print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
    "    if classToAnalyze == -1:\n",
    "        print('Out-of-activation pattern on the GTSRB test images: {} %'.format(100 * outofActivationPattern / total))\n",
    "        print('Out-of-activation pattern & misclassified / out-of-activation pattern : {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n",
    "    else:\n",
    "        print('Out-of-extended activation pattern on GTSRB test images, for class '+str(classToAnalyze)+':  {} %'.format(100 * outofActivationPattern / class_total[classToAnalyze]))\n",
    "        print('Out-of-activation pattern & misclassified / out-of-activation pattern for class '+str(classToAnalyze)+': {} %'.format(100 * outofActivationPatternAndResultWrong / (outofActivationPattern)))\n",
    "\n",
    "testOnAllClasses = False\n",
    "        \n",
    "if runIntervalMonitor: \n",
    "    testIntervalMonitor(testOnAllClasses, testloader, intervalmonitor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's further allow some fluctuation, and see how much are we gaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runIntervalMonitor:     \n",
    "    intervalmonitor.enlargeSetByOneBitFluctuation(stopSignClass)    \n",
    "    testIntervalMonitor(testOnAllClasses, testloader, intervalmonitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enlarge it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runIntervalMonitor:     \n",
    "    intervalmonitor.enlargeSetByOneBitFluctuation(stopSignClass)    \n",
    "    testIntervalMonitor(testOnAllClasses, testloader, intervalmonitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runIntervalMonitor:     \n",
    "    intervalmonitor.enlargeSetByOneBitFluctuation(stopSignClass)    \n",
    "    testIntervalMonitor(testOnAllClasses, testloader, intervalmonitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
